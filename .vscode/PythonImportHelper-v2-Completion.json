[
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "PatchCollection",
        "importPath": "matplotlib.collections",
        "description": "matplotlib.collections",
        "isExtraImport": true,
        "detail": "matplotlib.collections",
        "documentation": {}
    },
    {
        "label": "PatchCollection",
        "importPath": "matplotlib.collections",
        "description": "matplotlib.collections",
        "isExtraImport": true,
        "detail": "matplotlib.collections",
        "documentation": {}
    },
    {
        "label": "Polygon",
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "isExtraImport": true,
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "Polygon",
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "isExtraImport": true,
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "pycocotools._mask",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycocotools._mask",
        "description": "pycocotools._mask",
        "detail": "pycocotools._mask",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "Extension",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "Extension",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "CfgNode",
        "importPath": "detectron2.config",
        "description": "detectron2.config",
        "isExtraImport": true,
        "detail": "detectron2.config",
        "documentation": {}
    },
    {
        "label": "get_cfg",
        "importPath": "detectron2.config",
        "description": "detectron2.config",
        "isExtraImport": true,
        "detail": "detectron2.config",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "detection_utils",
        "importPath": "detectron2.data",
        "description": "detectron2.data",
        "isExtraImport": true,
        "detail": "detectron2.data",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "detectron2.data",
        "description": "detectron2.data",
        "isExtraImport": true,
        "detail": "detectron2.data",
        "documentation": {}
    },
    {
        "label": "MetadataCatalog",
        "importPath": "detectron2.data",
        "description": "detectron2.data",
        "isExtraImport": true,
        "detail": "detectron2.data",
        "documentation": {}
    },
    {
        "label": "build_detection_train_loader",
        "importPath": "detectron2.data",
        "description": "detectron2.data",
        "isExtraImport": true,
        "detail": "detectron2.data",
        "documentation": {}
    },
    {
        "label": "TransformGen",
        "importPath": "detectron2.data.transforms",
        "description": "detectron2.data.transforms",
        "isExtraImport": true,
        "detail": "detectron2.data.transforms",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "linear_sum_assignment",
        "importPath": "scipy.optimize",
        "description": "scipy.optimize",
        "isExtraImport": true,
        "detail": "scipy.optimize",
        "documentation": {}
    },
    {
        "label": "linear_sum_assignment",
        "importPath": "scipy.optimize",
        "description": "scipy.optimize",
        "isExtraImport": true,
        "detail": "scipy.optimize",
        "documentation": {}
    },
    {
        "label": "ShapeSpec",
        "importPath": "detectron2.layers",
        "description": "detectron2.layers",
        "isExtraImport": true,
        "detail": "detectron2.layers",
        "documentation": {}
    },
    {
        "label": "META_ARCH_REGISTRY",
        "importPath": "detectron2.modeling",
        "description": "detectron2.modeling",
        "isExtraImport": true,
        "detail": "detectron2.modeling",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "importPath": "detectron2.modeling",
        "description": "detectron2.modeling",
        "isExtraImport": true,
        "detail": "detectron2.modeling",
        "documentation": {}
    },
    {
        "label": "detector_postprocess",
        "importPath": "detectron2.modeling",
        "description": "detectron2.modeling",
        "isExtraImport": true,
        "detail": "detectron2.modeling",
        "documentation": {}
    },
    {
        "label": "Boxes",
        "importPath": "detectron2.structures",
        "description": "detectron2.structures",
        "isExtraImport": true,
        "detail": "detectron2.structures",
        "documentation": {}
    },
    {
        "label": "ImageList",
        "importPath": "detectron2.structures",
        "description": "detectron2.structures",
        "isExtraImport": true,
        "detail": "detectron2.structures",
        "documentation": {}
    },
    {
        "label": "Instances",
        "importPath": "detectron2.structures",
        "description": "detectron2.structures",
        "isExtraImport": true,
        "detail": "detectron2.structures",
        "documentation": {}
    },
    {
        "label": "BitMasks",
        "importPath": "detectron2.structures",
        "description": "detectron2.structures",
        "isExtraImport": true,
        "detail": "detectron2.structures",
        "documentation": {}
    },
    {
        "label": "PolygonMasks",
        "importPath": "detectron2.structures",
        "description": "detectron2.structures",
        "isExtraImport": true,
        "detail": "detectron2.structures",
        "documentation": {}
    },
    {
        "label": "log_first_n",
        "importPath": "detectron2.utils.logger",
        "description": "detectron2.utils.logger",
        "isExtraImport": true,
        "detail": "detectron2.utils.logger",
        "documentation": {}
    },
    {
        "label": "giou_loss",
        "importPath": "fvcore.nn",
        "description": "fvcore.nn",
        "isExtraImport": true,
        "detail": "fvcore.nn",
        "documentation": {}
    },
    {
        "label": "smooth_l1_loss",
        "importPath": "fvcore.nn",
        "description": "fvcore.nn",
        "isExtraImport": true,
        "detail": "fvcore.nn",
        "documentation": {}
    },
    {
        "label": "Joiner",
        "importPath": "models.backbone",
        "description": "models.backbone",
        "isExtraImport": true,
        "detail": "models.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "importPath": "models.backbone",
        "description": "models.backbone",
        "isExtraImport": true,
        "detail": "models.backbone",
        "documentation": {}
    },
    {
        "label": "Joiner",
        "importPath": "models.backbone",
        "description": "models.backbone",
        "isExtraImport": true,
        "detail": "models.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "importPath": "models.backbone",
        "description": "models.backbone",
        "isExtraImport": true,
        "detail": "models.backbone",
        "documentation": {}
    },
    {
        "label": "Joiner",
        "importPath": "models.backbone",
        "description": "models.backbone",
        "isExtraImport": true,
        "detail": "models.backbone",
        "documentation": {}
    },
    {
        "label": "BackboneBase",
        "importPath": "models.backbone",
        "description": "models.backbone",
        "isExtraImport": true,
        "detail": "models.backbone",
        "documentation": {}
    },
    {
        "label": "DETR",
        "importPath": "models.detr",
        "description": "models.detr",
        "isExtraImport": true,
        "detail": "models.detr",
        "documentation": {}
    },
    {
        "label": "SetCriterion",
        "importPath": "models.detr",
        "description": "models.detr",
        "isExtraImport": true,
        "detail": "models.detr",
        "documentation": {}
    },
    {
        "label": "DETR",
        "importPath": "models.detr",
        "description": "models.detr",
        "isExtraImport": true,
        "detail": "models.detr",
        "documentation": {}
    },
    {
        "label": "PostProcess",
        "importPath": "models.detr",
        "description": "models.detr",
        "isExtraImport": true,
        "detail": "models.detr",
        "documentation": {}
    },
    {
        "label": "HungarianMatcher",
        "importPath": "models.matcher",
        "description": "models.matcher",
        "isExtraImport": true,
        "detail": "models.matcher",
        "documentation": {}
    },
    {
        "label": "HungarianMatcher",
        "importPath": "models.matcher",
        "description": "models.matcher",
        "isExtraImport": true,
        "detail": "models.matcher",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine",
        "importPath": "models.position_encoding",
        "description": "models.position_encoding",
        "isExtraImport": true,
        "detail": "models.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine",
        "importPath": "models.position_encoding",
        "description": "models.position_encoding",
        "isExtraImport": true,
        "detail": "models.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine",
        "importPath": "models.position_encoding",
        "description": "models.position_encoding",
        "isExtraImport": true,
        "detail": "models.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingLearned",
        "importPath": "models.position_encoding",
        "description": "models.position_encoding",
        "isExtraImport": true,
        "detail": "models.position_encoding",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "models.transformer",
        "description": "models.transformer",
        "isExtraImport": true,
        "detail": "models.transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "models.transformer",
        "description": "models.transformer",
        "isExtraImport": true,
        "detail": "models.transformer",
        "documentation": {}
    },
    {
        "label": "DETRsegm",
        "importPath": "models.segmentation",
        "description": "models.segmentation",
        "isExtraImport": true,
        "detail": "models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessPanoptic",
        "importPath": "models.segmentation",
        "description": "models.segmentation",
        "isExtraImport": true,
        "detail": "models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessSegm",
        "importPath": "models.segmentation",
        "description": "models.segmentation",
        "isExtraImport": true,
        "detail": "models.segmentation",
        "documentation": {}
    },
    {
        "label": "DETRsegm",
        "importPath": "models.segmentation",
        "description": "models.segmentation",
        "isExtraImport": true,
        "detail": "models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessPanoptic",
        "importPath": "models.segmentation",
        "description": "models.segmentation",
        "isExtraImport": true,
        "detail": "models.segmentation",
        "documentation": {}
    },
    {
        "label": "util.box_ops",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "masks_to_boxes",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "importPath": "util.box_ops",
        "description": "util.box_ops",
        "isExtraImport": true,
        "detail": "util.box_ops",
        "documentation": {}
    },
    {
        "label": "util.misc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "util.misc",
        "description": "util.misc",
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "importPath": "util.misc",
        "description": "util.misc",
        "isExtraImport": true,
        "detail": "util.misc",
        "documentation": {}
    },
    {
        "label": "convert_coco_poly_to_mask",
        "importPath": "datasets.coco",
        "description": "datasets.coco",
        "isExtraImport": true,
        "detail": "datasets.coco",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "detectron2.utils.comm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "detectron2.utils.comm",
        "description": "detectron2.utils.comm",
        "detail": "detectron2.utils.comm",
        "documentation": {}
    },
    {
        "label": "DetrDatasetMapper",
        "importPath": "d2.detr",
        "description": "d2.detr",
        "isExtraImport": true,
        "detail": "d2.detr",
        "documentation": {}
    },
    {
        "label": "add_detr_config",
        "importPath": "d2.detr",
        "description": "d2.detr",
        "isExtraImport": true,
        "detail": "d2.detr",
        "documentation": {}
    },
    {
        "label": "DetectionCheckpointer",
        "importPath": "detectron2.checkpoint",
        "description": "detectron2.checkpoint",
        "isExtraImport": true,
        "detail": "detectron2.checkpoint",
        "documentation": {}
    },
    {
        "label": "DefaultTrainer",
        "importPath": "detectron2.engine",
        "description": "detectron2.engine",
        "isExtraImport": true,
        "detail": "detectron2.engine",
        "documentation": {}
    },
    {
        "label": "default_argument_parser",
        "importPath": "detectron2.engine",
        "description": "detectron2.engine",
        "isExtraImport": true,
        "detail": "detectron2.engine",
        "documentation": {}
    },
    {
        "label": "default_setup",
        "importPath": "detectron2.engine",
        "description": "detectron2.engine",
        "isExtraImport": true,
        "detail": "detectron2.engine",
        "documentation": {}
    },
    {
        "label": "launch",
        "importPath": "detectron2.engine",
        "description": "detectron2.engine",
        "isExtraImport": true,
        "detail": "detectron2.engine",
        "documentation": {}
    },
    {
        "label": "COCOEvaluator",
        "importPath": "detectron2.evaluation",
        "description": "detectron2.evaluation",
        "isExtraImport": true,
        "detail": "detectron2.evaluation",
        "documentation": {}
    },
    {
        "label": "verify_results",
        "importPath": "detectron2.evaluation",
        "description": "detectron2.evaluation",
        "isExtraImport": true,
        "detail": "detectron2.evaluation",
        "documentation": {}
    },
    {
        "label": "maybe_add_gradient_clipping",
        "importPath": "detectron2.solver.build",
        "description": "detectron2.solver.build",
        "isExtraImport": true,
        "detail": "detectron2.solver.build",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "PurePath",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "mask",
        "importPath": "pycocotools",
        "description": "pycocotools",
        "isExtraImport": true,
        "detail": "pycocotools",
        "documentation": {}
    },
    {
        "label": "datasets.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets.transforms",
        "description": "datasets.transforms",
        "detail": "datasets.transforms",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "COCOeval",
        "importPath": "pycocotools.cocoeval",
        "description": "pycocotools.cocoeval",
        "isExtraImport": true,
        "detail": "pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "COCO",
        "importPath": "pycocotools.coco",
        "description": "pycocotools.coco",
        "isExtraImport": true,
        "detail": "pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "pycocotools.mask",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycocotools.mask",
        "description": "pycocotools.mask",
        "detail": "pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "IdGenerator",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "id2rgb",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "get_traceback",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "IdGenerator",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "get_traceback",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "IdGenerator",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "get_traceback",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "IdGenerator",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "get_traceback",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "get_traceback",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "IdGenerator",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "id2rgb",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "get_traceback",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "IdGenerator",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "importPath": "panopticapi.utils",
        "description": "panopticapi.utils",
        "isExtraImport": true,
        "detail": "panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "IntermediateLayerGetter",
        "importPath": "torchvision.models._utils",
        "description": "torchvision.models._utils",
        "isExtraImport": true,
        "detail": "torchvision.models._utils",
        "documentation": {}
    },
    {
        "label": "box_ops",
        "importPath": "util",
        "description": "util",
        "isExtraImport": true,
        "detail": "util",
        "documentation": {}
    },
    {
        "label": "box_ops",
        "importPath": "util",
        "description": "util",
        "isExtraImport": true,
        "detail": "util",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "PIL.Image",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.Image",
        "description": "PIL.Image",
        "detail": "PIL.Image",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "find_boundaries",
        "importPath": "skimage.segmentation",
        "description": "skimage.segmentation",
        "isExtraImport": true,
        "detail": "skimage.segmentation",
        "documentation": {}
    },
    {
        "label": "box_area",
        "importPath": "torchvision.ops.boxes",
        "description": "torchvision.ops.boxes",
        "isExtraImport": true,
        "detail": "torchvision.ops.boxes",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "CocoEvaluator",
        "importPath": "datasets.coco_eval",
        "description": "datasets.coco_eval",
        "isExtraImport": true,
        "detail": "datasets.coco_eval",
        "documentation": {}
    },
    {
        "label": "PanopticEvaluator",
        "importPath": "datasets.panoptic_eval",
        "description": "datasets.panoptic_eval",
        "isExtraImport": true,
        "detail": "datasets.panoptic_eval",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "get_coco_api_from_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "engine",
        "description": "engine",
        "isExtraImport": true,
        "detail": "engine",
        "documentation": {}
    },
    {
        "label": "train_one_epoch",
        "importPath": "engine",
        "description": "engine",
        "isExtraImport": true,
        "detail": "engine",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "main",
        "description": "main",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "submitit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "submitit",
        "description": "submitit",
        "detail": "submitit",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "detr_resnet50",
        "importPath": "hubconf",
        "description": "hubconf",
        "isExtraImport": true,
        "detail": "hubconf",
        "documentation": {}
    },
    {
        "label": "detr_resnet50_panoptic",
        "importPath": "hubconf",
        "description": "hubconf",
        "isExtraImport": true,
        "detail": "hubconf",
        "documentation": {}
    },
    {
        "label": "COCO",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "peekOfCode": "class COCO:\n    def __init__(self, annotation_file=None):\n        \"\"\"\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        \"\"\"\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "peekOfCode": "__author__ = 'tylin'\n__version__ = '2.0'\n# Interface for accessing the Microsoft COCO dataset.\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "__version__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "peekOfCode": "__version__ = '2.0'\n# Interface for accessing the Microsoft COCO dataset.\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "PYTHON_VERSION",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "peekOfCode": "PYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\ndef _isArrayLike(obj):\n    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')\nclass COCO:\n    def __init__(self, annotation_file=None):\n        \"\"\"",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "COCOeval",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "peekOfCode": "class COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "Params",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "peekOfCode": "class Params:\n    '''\n    Params for coco evaluation api\n    '''\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, int(np.round((1.00 - .0) / .01)) + 1, endpoint=True)",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "peekOfCode": "__author__ = 'tsungyi'\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "encode",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "peekOfCode": "def encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order='F'))[0]\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "decode",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "peekOfCode": "def decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "peekOfCode": "def area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "toBbox",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "peekOfCode": "def toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "peekOfCode": "__author__ = 'tsungyi'\nimport pycocotools._mask as _mask\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "frPyObjects",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "peekOfCode": "frPyObjects = _mask.frPyObjects\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order='F'))[0]\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.build.lib.linux-x86_64-cpython-38.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "COCO",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "peekOfCode": "class COCO:\n    def __init__(self, annotation_file=None):\n        \"\"\"\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        \"\"\"\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "peekOfCode": "__author__ = 'tylin'\n__version__ = '2.0'\n# Interface for accessing the Microsoft COCO dataset.\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "__version__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "peekOfCode": "__version__ = '2.0'\n# Interface for accessing the Microsoft COCO dataset.\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "PYTHON_VERSION",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "peekOfCode": "PYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\ndef _isArrayLike(obj):\n    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')\nclass COCO:\n    def __init__(self, annotation_file=None):\n        \"\"\"",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.coco",
        "documentation": {}
    },
    {
        "label": "COCOeval",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "peekOfCode": "class COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "Params",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "peekOfCode": "class Params:\n    '''\n    Params for coco evaluation api\n    '''\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, int(np.round((1.00 - .0) / .01)) + 1, endpoint=True)",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "peekOfCode": "__author__ = 'tsungyi'\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.cocoeval",
        "documentation": {}
    },
    {
        "label": "encode",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "peekOfCode": "def encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order='F'))[0]\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "decode",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "peekOfCode": "def decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "peekOfCode": "def area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "toBbox",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "peekOfCode": "def toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "__author__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "peekOfCode": "__author__ = 'tsungyi'\nimport pycocotools._mask as _mask\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "frPyObjects",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "peekOfCode": "frPyObjects = _mask.frPyObjects\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order='F'))[0]\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.pycocotools.mask",
        "documentation": {}
    },
    {
        "label": "ext_modules",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.setup",
        "description": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.setup",
        "peekOfCode": "ext_modules = [\n    Extension(\n        'pycocotools._mask',\n        sources=['../common/maskApi.c', 'pycocotools/_mask.pyx'],\n        include_dirs = [np.get_include(), '../common'],\n        extra_compile_args=['-Wno-cpp', '-Wno-unused-function', '-std=c99'],\n    )\n]\nsetup(\n    name='pycocotools',",
        "detail": "my_work.Project 1 Vision Transformer.cocoapi.PythonAPI.setup",
        "documentation": {}
    },
    {
        "label": "add_detr_config",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.d2.detr.config",
        "description": "my_work.Project 1 Vision Transformer.d2.detr.config",
        "peekOfCode": "def add_detr_config(cfg):\n    \"\"\"\n    Add config for DETR.\n    \"\"\"\n    cfg.MODEL.DETR = CN()\n    cfg.MODEL.DETR.NUM_CLASSES = 80\n    # For Segmentation\n    cfg.MODEL.DETR.FROZEN_WEIGHTS = ''\n    # LOSS\n    cfg.MODEL.DETR.GIOU_WEIGHT = 2.0",
        "detail": "my_work.Project 1 Vision Transformer.d2.detr.config",
        "documentation": {}
    },
    {
        "label": "DetrDatasetMapper",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "description": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "peekOfCode": "class DetrDatasetMapper:\n    \"\"\"\n    A callable which takes a dataset dict in Detectron2 Dataset format,\n    and map it into a format used by DETR.\n    The callable currently does the following:\n    1. Read the image from \"file_name\"\n    2. Applies geometric transforms to the image and annotation\n    3. Find and applies suitable cropping to the image and annotation\n    4. Prepare image and annotation to Tensors\n    \"\"\"",
        "detail": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "documentation": {}
    },
    {
        "label": "build_transform_gen",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "description": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "peekOfCode": "def build_transform_gen(cfg, is_train):\n    \"\"\"\n    Create a list of :class:`TransformGen` from config.\n    Returns:\n        list[TransformGen]\n    \"\"\"\n    if is_train:\n        min_size = cfg.INPUT.MIN_SIZE_TRAIN\n        max_size = cfg.INPUT.MAX_SIZE_TRAIN\n        sample_style = cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING",
        "detail": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "description": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "peekOfCode": "__all__ = [\"DetrDatasetMapper\"]\ndef build_transform_gen(cfg, is_train):\n    \"\"\"\n    Create a list of :class:`TransformGen` from config.\n    Returns:\n        list[TransformGen]\n    \"\"\"\n    if is_train:\n        min_size = cfg.INPUT.MIN_SIZE_TRAIN\n        max_size = cfg.INPUT.MAX_SIZE_TRAIN",
        "detail": "my_work.Project 1 Vision Transformer.d2.detr.dataset_mapper",
        "documentation": {}
    },
    {
        "label": "MaskedBackbone",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "description": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "peekOfCode": "class MaskedBackbone(nn.Module):\n    \"\"\" This is a thin wrapper around D2's backbone to provide padding masking\"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.backbone = build_backbone(cfg)\n        backbone_shape = self.backbone.output_shape()\n        self.feature_strides = [backbone_shape[f].stride for f in backbone_shape.keys()]\n        self.num_channels = backbone_shape[list(backbone_shape.keys())[-1]].channels\n    def forward(self, images):\n        features = self.backbone(images.tensor)",
        "detail": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "documentation": {}
    },
    {
        "label": "Detr",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "description": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "peekOfCode": "class Detr(nn.Module):\n    \"\"\"\n    Implement Detr\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.device = torch.device(cfg.MODEL.DEVICE)\n        self.num_classes = cfg.MODEL.DETR.NUM_CLASSES\n        self.mask_on = cfg.MODEL.MASK_ON\n        hidden_dim = cfg.MODEL.DETR.HIDDEN_DIM",
        "detail": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "description": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "peekOfCode": "__all__ = [\"Detr\"]\nclass MaskedBackbone(nn.Module):\n    \"\"\" This is a thin wrapper around D2's backbone to provide padding masking\"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.backbone = build_backbone(cfg)\n        backbone_shape = self.backbone.output_shape()\n        self.feature_strides = [backbone_shape[f].stride for f in backbone_shape.keys()]\n        self.num_channels = backbone_shape[list(backbone_shape.keys())[-1]].channels\n    def forward(self, images):",
        "detail": "my_work.Project 1 Vision Transformer.d2.detr.detr",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.d2.converter",
        "description": "my_work.Project 1 Vision Transformer.d2.converter",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\"D2 model converter\")\n    parser.add_argument(\"--source_model\", default=\"\", type=str, help=\"Path or url to the DETR model to convert\")\n    parser.add_argument(\"--output_model\", default=\"\", type=str, help=\"Path where to save the converted model\")\n    return parser.parse_args()\ndef main():\n    args = parse_args()\n    # D2 expects contiguous classes, so we need to remap the 92 classes from DETR\n    # fmt: off\n    coco_idx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,",
        "detail": "my_work.Project 1 Vision Transformer.d2.converter",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.d2.converter",
        "description": "my_work.Project 1 Vision Transformer.d2.converter",
        "peekOfCode": "def main():\n    args = parse_args()\n    # D2 expects contiguous classes, so we need to remap the 92 classes from DETR\n    # fmt: off\n    coco_idx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n                27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51,\n                52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77,\n                78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91]\n    # fmt: on\n    coco_idx = np.array(coco_idx)",
        "detail": "my_work.Project 1 Vision Transformer.d2.converter",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.d2.train_net",
        "description": "my_work.Project 1 Vision Transformer.d2.train_net",
        "peekOfCode": "class Trainer(DefaultTrainer):\n    \"\"\"\n    Extension of the Trainer class adapted to DETR.\n    \"\"\"\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        \"\"\"\n        Create evaluator(s) for a given dataset.\n        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n        For your own dataset, you can simply create an evaluator manually in your",
        "detail": "my_work.Project 1 Vision Transformer.d2.train_net",
        "documentation": {}
    },
    {
        "label": "setup",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.d2.train_net",
        "description": "my_work.Project 1 Vision Transformer.d2.train_net",
        "peekOfCode": "def setup(args):\n    \"\"\"\n    Create configs and perform basic setups.\n    \"\"\"\n    cfg = get_cfg()\n    add_detr_config(cfg)\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n    default_setup(cfg, args)",
        "detail": "my_work.Project 1 Vision Transformer.d2.train_net",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.d2.train_net",
        "description": "my_work.Project 1 Vision Transformer.d2.train_net",
        "peekOfCode": "def main(args):\n    cfg = setup(args)\n    if args.eval_only:\n        model = Trainer.build_model(cfg)\n        DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(cfg.MODEL.WEIGHTS, resume=args.resume)\n        res = Trainer.test(cfg, model)\n        if comm.is_main_process():\n            verify_results(cfg, res)\n        return res\n    trainer = Trainer(cfg)",
        "detail": "my_work.Project 1 Vision Transformer.d2.train_net",
        "documentation": {}
    },
    {
        "label": "CocoDetection",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco",
        "peekOfCode": "class CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, ann_file, transforms, return_masks):\n        super(CocoDetection, self).__init__(img_folder, ann_file)\n        self._transforms = transforms\n        self.prepare = ConvertCocoPolysToMask(return_masks)\n    def __getitem__(self, idx):\n        img, target = super(CocoDetection, self).__getitem__(idx)\n        image_id = self.ids[idx]\n        target = {'image_id': image_id, 'annotations': target}\n        img, target = self.prepare(img, target)",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco",
        "documentation": {}
    },
    {
        "label": "ConvertCocoPolysToMask",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco",
        "peekOfCode": "class ConvertCocoPolysToMask(object):\n    def __init__(self, return_masks=False):\n        self.return_masks = return_masks\n    def __call__(self, image, target):\n        w, h = image.size\n        image_id = target[\"image_id\"]\n        image_id = torch.tensor([image_id])\n        anno = target[\"annotations\"]\n        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n        boxes = [obj[\"bbox\"] for obj in anno]",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco",
        "documentation": {}
    },
    {
        "label": "convert_coco_poly_to_mask",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco",
        "peekOfCode": "def convert_coco_poly_to_mask(segmentations, height, width):\n    masks = []\n    for polygons in segmentations:\n        rles = coco_mask.frPyObjects(polygons, height, width)\n        mask = coco_mask.decode(rles)\n        if len(mask.shape) < 3:\n            mask = mask[..., None]\n        mask = torch.as_tensor(mask, dtype=torch.uint8)\n        mask = mask.any(dim=2)\n        masks.append(mask)",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco",
        "documentation": {}
    },
    {
        "label": "make_coco_transforms",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco",
        "peekOfCode": "def make_coco_transforms(image_set):\n    normalize = T.Compose([\n        T.ToTensor(),\n        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n    if image_set == 'train':\n        return T.Compose([\n            T.RandomHorizontalFlip(),\n            T.RandomSelect(",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco",
        "peekOfCode": "def build(image_set, args):\n    root = Path(args.coco_path)\n    assert root.exists(), f'provided COCO path {root} does not exist'\n    mode = 'instances'\n    PATHS = {\n        \"train\": (root / \"train2017\", root / \"annotations\" / f'{mode}_train2017.json'),\n        \"val\": (root / \"val2017\", root / \"annotations\" / f'{mode}_val2017.json'),\n    }\n    img_folder, ann_file = PATHS[image_set]\n    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=args.masks)",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco",
        "documentation": {}
    },
    {
        "label": "CocoEvaluator",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "peekOfCode": "class CocoEvaluator(object):\n    def __init__(self, coco_gt, iou_types):\n        assert isinstance(iou_types, (list, tuple))\n        coco_gt = copy.deepcopy(coco_gt)\n        self.coco_gt = coco_gt\n        self.iou_types = iou_types\n        self.coco_eval = {}\n        for iou_type in iou_types:\n            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n        self.img_ids = []",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "documentation": {}
    },
    {
        "label": "convert_to_xywh",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "peekOfCode": "def convert_to_xywh(boxes):\n    xmin, ymin, xmax, ymax = boxes.unbind(1)\n    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\ndef merge(img_ids, eval_imgs):\n    all_img_ids = all_gather(img_ids)\n    all_eval_imgs = all_gather(eval_imgs)\n    merged_img_ids = []\n    for p in all_img_ids:\n        merged_img_ids.extend(p)\n    merged_eval_imgs = []",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "documentation": {}
    },
    {
        "label": "merge",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "peekOfCode": "def merge(img_ids, eval_imgs):\n    all_img_ids = all_gather(img_ids)\n    all_eval_imgs = all_gather(eval_imgs)\n    merged_img_ids = []\n    for p in all_img_ids:\n        merged_img_ids.extend(p)\n    merged_eval_imgs = []\n    for p in all_eval_imgs:\n        merged_eval_imgs.append(p)\n    merged_img_ids = np.array(merged_img_ids)",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "documentation": {}
    },
    {
        "label": "create_common_coco_eval",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "peekOfCode": "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n    img_ids = list(img_ids)\n    eval_imgs = list(eval_imgs.flatten())\n    coco_eval.evalImgs = eval_imgs\n    coco_eval.params.imgIds = img_ids\n    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n#################################################################\n# From pycocotools, just removed the prints and fixed\n# a Python3 bug about unicode not defined",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "peekOfCode": "def evaluate(self):\n    '''\n    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n    :return: None\n    '''\n    # tic = time.time()\n    # print('Running per image evaluation...')\n    p = self.params\n    # add backward compatibility if useSegm is specified in params\n    if p.useSegm is not None:",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco_eval",
        "documentation": {}
    },
    {
        "label": "CocoPanoptic",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco_panoptic",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco_panoptic",
        "peekOfCode": "class CocoPanoptic:\n    def __init__(self, img_folder, ann_folder, ann_file, transforms=None, return_masks=True):\n        with open(ann_file, 'r') as f:\n            self.coco = json.load(f)\n        # sort 'images' field so that they are aligned with 'annotations'\n        # i.e., in alphabetical order\n        self.coco['images'] = sorted(self.coco['images'], key=lambda x: x['id'])\n        # sanity check\n        if \"annotations\" in self.coco:\n            for img, ann in zip(self.coco['images'], self.coco['annotations']):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco_panoptic",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.coco_panoptic",
        "description": "my_work.Project 1 Vision Transformer.datasets.coco_panoptic",
        "peekOfCode": "def build(image_set, args):\n    img_folder_root = Path(args.coco_path)\n    ann_folder_root = Path(args.coco_panoptic_path)\n    assert img_folder_root.exists(), f'provided COCO path {img_folder_root} does not exist'\n    assert ann_folder_root.exists(), f'provided COCO path {ann_folder_root} does not exist'\n    mode = 'panoptic'\n    PATHS = {\n        \"train\": (\"train2017\", Path(\"annotations\") / f'{mode}_train2017.json'),\n        \"val\": (\"val2017\", Path(\"annotations\") / f'{mode}_val2017.json'),\n    }",
        "detail": "my_work.Project 1 Vision Transformer.datasets.coco_panoptic",
        "documentation": {}
    },
    {
        "label": "PanopticEvaluator",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.panoptic_eval",
        "description": "my_work.Project 1 Vision Transformer.datasets.panoptic_eval",
        "peekOfCode": "class PanopticEvaluator(object):\n    def __init__(self, ann_file, ann_folder, output_dir=\"panoptic_eval\"):\n        self.gt_json = ann_file\n        self.gt_folder = ann_folder\n        if utils.is_main_process():\n            if not os.path.exists(output_dir):\n                os.mkdir(output_dir)\n        self.output_dir = output_dir\n        self.predictions = []\n    def update(self, predictions):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.panoptic_eval",
        "documentation": {}
    },
    {
        "label": "RandomCrop",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class RandomCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        region = T.RandomCrop.get_params(img, self.size)\n        return crop(img, target, region)\nclass RandomSizeCrop(object):\n    def __init__(self, min_size: int, max_size: int):\n        self.min_size = min_size\n        self.max_size = max_size",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomSizeCrop",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class RandomSizeCrop(object):\n    def __init__(self, min_size: int, max_size: int):\n        self.min_size = min_size\n        self.max_size = max_size\n    def __call__(self, img: PIL.Image.Image, target: dict):\n        w = random.randint(self.min_size, min(img.width, self.max_size))\n        h = random.randint(self.min_size, min(img.height, self.max_size))\n        region = T.RandomCrop.get_params(img, [h, w])\n        return crop(img, target, region)\nclass CenterCrop(object):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "CenterCrop",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class CenterCrop(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, img, target):\n        image_width, image_height = img.size\n        crop_height, crop_width = self.size\n        crop_top = int(round((image_height - crop_height) / 2.))\n        crop_left = int(round((image_width - crop_width) / 2.))\n        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\nclass RandomHorizontalFlip(object):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomHorizontalFlip",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, img, target):\n        if random.random() < self.p:\n            return hflip(img, target)\n        return img, target\nclass RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomResize",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class RandomResize(object):\n    def __init__(self, sizes, max_size=None):\n        assert isinstance(sizes, (list, tuple))\n        self.sizes = sizes\n        self.max_size = max_size\n    def __call__(self, img, target=None):\n        size = random.choice(self.sizes)\n        return resize(img, target, size, self.max_size)\nclass RandomPad(object):\n    def __init__(self, max_pad):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomPad",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class RandomPad(object):\n    def __init__(self, max_pad):\n        self.max_pad = max_pad\n    def __call__(self, img, target):\n        pad_x = random.randint(0, self.max_pad)\n        pad_y = random.randint(0, self.max_pad)\n        return pad(img, target, (pad_x, pad_y))\nclass RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomSelect",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class RandomSelect(object):\n    \"\"\"\n    Randomly selects between transforms1 and transforms2,\n    with probability p for transforms1 and (1 - p) for transforms2\n    \"\"\"\n    def __init__(self, transforms1, transforms2, p=0.5):\n        self.transforms1 = transforms1\n        self.transforms2 = transforms2\n        self.p = p\n    def __call__(self, img, target):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class ToTensor(object):\n    def __call__(self, img, target):\n        return F.to_tensor(img), target\nclass RandomErasing(object):\n    def __init__(self, *args, **kwargs):\n        self.eraser = T.RandomErasing(*args, **kwargs)\n    def __call__(self, img, target):\n        return self.eraser(img), target\nclass Normalize(object):\n    def __init__(self, mean, std):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomErasing",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class RandomErasing(object):\n    def __init__(self, *args, **kwargs):\n        self.eraser = T.RandomErasing(*args, **kwargs)\n    def __call__(self, img, target):\n        return self.eraser(img), target\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, image, target=None):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def __call__(self, image, target=None):\n        image = F.normalize(image, mean=self.mean, std=self.std)\n        if target is None:\n            return image, None\n        target = target.copy()\n        h, w = image.shape[-2:]",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \"(\"\n        for t in self.transforms:",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "crop",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "def crop(image, target, region):\n    cropped_image = F.crop(image, *region)\n    target = target.copy()\n    i, j, h, w = region\n    # should we do something wrt the original size?\n    target[\"size\"] = torch.tensor([h, w])\n    fields = [\"labels\", \"area\", \"iscrowd\"]\n    if \"boxes\" in target:\n        boxes = target[\"boxes\"]\n        max_size = torch.as_tensor([w, h], dtype=torch.float32)",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "hflip",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "def hflip(image, target):\n    flipped_image = F.hflip(image)\n    w, h = image.size\n    target = target.copy()\n    if \"boxes\" in target:\n        boxes = target[\"boxes\"]\n        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n        target[\"boxes\"] = boxes\n    if \"masks\" in target:\n        target['masks'] = target['masks'].flip(-1)",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "def resize(image, target, size, max_size=None):\n    # size can be min_size (scalar) or (w, h) tuple\n    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n        w, h = image_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(round(max_size * min_original_size / max_original_size))\n        if (w <= h and w == size) or (h <= w and h == size):",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "pad",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "description": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "peekOfCode": "def pad(image, target, padding):\n    # assumes that we only pad on the bottom right corners\n    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n    if target is None:\n        return padded_image, None\n    target = target.copy()\n    # should we do something wrt the original size?\n    target[\"size\"] = torch.tensor(padded_image.size[::-1])\n    if \"masks\" in target:\n        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))",
        "detail": "my_work.Project 1 Vision Transformer.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "FrozenBatchNorm2d",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.backbone",
        "description": "my_work.Project 1 Vision Transformer.models.backbone",
        "peekOfCode": "class FrozenBatchNorm2d(torch.nn.Module):\n    \"\"\"\n    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n    without which any other models than torchvision.models.resnet[18,34,50,101]\n    produce nans.\n    \"\"\"\n    def __init__(self, n):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer(\"weight\", torch.ones(n))",
        "detail": "my_work.Project 1 Vision Transformer.models.backbone",
        "documentation": {}
    },
    {
        "label": "BackboneBase",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.backbone",
        "description": "my_work.Project 1 Vision Transformer.models.backbone",
        "peekOfCode": "class BackboneBase(nn.Module):\n    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n        super().__init__()\n        for name, parameter in backbone.named_parameters():\n            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n                parameter.requires_grad_(False)\n        ## 是否返回中间层，在多尺度融合操作时会用到\n        if return_interm_layers:\n            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n        else:",
        "detail": "my_work.Project 1 Vision Transformer.models.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.backbone",
        "description": "my_work.Project 1 Vision Transformer.models.backbone",
        "peekOfCode": "class Backbone(BackboneBase):\n    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n    def __init__(self, name: str,\n                 train_backbone: bool,\n                 return_interm_layers: bool,\n                 dilation: bool):\n        backbone = getattr(torchvision.models, name)(\n            replace_stride_with_dilation=[False, False, dilation],\n            pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d)\n        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048",
        "detail": "my_work.Project 1 Vision Transformer.models.backbone",
        "documentation": {}
    },
    {
        "label": "Joiner",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.backbone",
        "description": "my_work.Project 1 Vision Transformer.models.backbone",
        "peekOfCode": "class Joiner(nn.Sequential):\n    def __init__(self, backbone, position_embedding):\n        super().__init__(backbone, position_embedding)\n    def forward(self, tensor_list: NestedTensor):\n        # tensor_list: pad预处理之后的图像信息\n        # self[0]: 对应backbone\n        xs = self[0](tensor_list)\n        ## 初始化一个空列表 out\n        out: List[NestedTensor] = []\n        pos = []",
        "detail": "my_work.Project 1 Vision Transformer.models.backbone",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.models.backbone",
        "description": "my_work.Project 1 Vision Transformer.models.backbone",
        "peekOfCode": "def build_backbone(args):\n    train_backbone = args.lr_backbone > 0\n    return_interm_layers = args.masks\n    backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n    position_embedding = build_position_encoding(args)\n    model = Joiner(backbone, position_embedding)\n    model.num_channels = backbone.num_channels\n    return model",
        "detail": "my_work.Project 1 Vision Transformer.models.backbone",
        "documentation": {}
    },
    {
        "label": "DETR",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.detr",
        "description": "my_work.Project 1 Vision Transformer.models.detr",
        "peekOfCode": "class DETR(nn.Module):\n    \"\"\" This is the DETR module that performs object detection \"\"\"\n    def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n        \"\"\" Initializes the model.\n        Parameters:\n            backbone: torch module of the backbone to be used. See backbone.py\n            transformer: torch module of the transformer architecture. See transformer.py\n            num_classes: number of object classes\n            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n                         DETR can detect in a single image. For COCO, we recommend 100 queries.",
        "detail": "my_work.Project 1 Vision Transformer.models.detr",
        "documentation": {}
    },
    {
        "label": "SetCriterion",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.detr",
        "description": "my_work.Project 1 Vision Transformer.models.detr",
        "peekOfCode": "class SetCriterion(nn.Module):\n    \"\"\" This class computes the loss for DETR.\n    The process happens in two steps:\n        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n    \"\"\"\n    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n        \"\"\" Create the criterion.\n        Parameters:\n            num_classes: number of object categories, omitting the special no-object category",
        "detail": "my_work.Project 1 Vision Transformer.models.detr",
        "documentation": {}
    },
    {
        "label": "PostProcess",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.detr",
        "description": "my_work.Project 1 Vision Transformer.models.detr",
        "peekOfCode": "class PostProcess(nn.Module):\n    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n    @torch.no_grad()\n    def forward(self, outputs, target_sizes):\n        \"\"\" Perform the computation\n        Parameters:\n            outputs: raw outputs of the model\n            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n                          For evaluation, this must be the original image size (before any data augmentation)\n                          For visualization, this should be the image size after data augment, but before padding",
        "detail": "my_work.Project 1 Vision Transformer.models.detr",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.detr",
        "description": "my_work.Project 1 Vision Transformer.models.detr",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)",
        "detail": "my_work.Project 1 Vision Transformer.models.detr",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.models.detr",
        "description": "my_work.Project 1 Vision Transformer.models.detr",
        "peekOfCode": "def build(args):\n    # the `num_classes` naming here is somewhat misleading.\n    # it indeed corresponds to `max_obj_id + 1`, where max_obj_id\n    # is the maximum id for a class in your dataset. For example,\n    # COCO has a max_obj_id of 90, so we pass `num_classes` to be 91.\n    # As another example, for a dataset that has a single class with id 1,\n    # you should pass `num_classes` to be 2 (max_obj_id + 1).\n    # For more details on this, check the following discussion\n    # https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\n    num_classes = 20 if args.dataset_file != 'coco' else 91",
        "detail": "my_work.Project 1 Vision Transformer.models.detr",
        "documentation": {}
    },
    {
        "label": "HungarianMatcher",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.matcher",
        "description": "my_work.Project 1 Vision Transformer.models.matcher",
        "peekOfCode": "class HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n        \"\"\"Creates the matcher\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost",
        "detail": "my_work.Project 1 Vision Transformer.models.matcher",
        "documentation": {}
    },
    {
        "label": "build_matcher",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.models.matcher",
        "description": "my_work.Project 1 Vision Transformer.models.matcher",
        "peekOfCode": "def build_matcher(args):\n    return HungarianMatcher(cost_class=args.set_cost_class, cost_bbox=args.set_cost_bbox, cost_giou=args.set_cost_giou)",
        "detail": "my_work.Project 1 Vision Transformer.models.matcher",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "description": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "peekOfCode": "class PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize",
        "detail": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingLearned",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "description": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "peekOfCode": "class PositionEmbeddingLearned(nn.Module):\n    \"\"\"\n    Absolute pos embedding, learned.\n    \"\"\"\n    def __init__(self, num_pos_feats=256):\n        super().__init__()\n        self.row_embed = nn.Embedding(50, num_pos_feats)\n        self.col_embed = nn.Embedding(50, num_pos_feats)\n        self.reset_parameters()\n    def reset_parameters(self):",
        "detail": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "build_position_encoding",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "description": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "peekOfCode": "def build_position_encoding(args):\n    N_steps = args.hidden_dim // 2\n    if args.position_embedding in ('v2', 'sine'):\n        # TODO find a better way of exposing other arguments\n        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n    elif args.position_embedding in ('v3', 'learned'):\n        position_embedding = PositionEmbeddingLearned(N_steps)\n    else:\n        raise ValueError(f\"not supported {args.position_embedding}\")\n    return position_embedding",
        "detail": "my_work.Project 1 Vision Transformer.models.position_encoding",
        "documentation": {}
    },
    {
        "label": "DETRsegm",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.segmentation",
        "description": "my_work.Project 1 Vision Transformer.models.segmentation",
        "peekOfCode": "class DETRsegm(nn.Module):\n    def __init__(self, detr, freeze_detr=False):\n        super().__init__()\n        self.detr = detr\n        if freeze_detr:\n            for p in self.parameters():\n                p.requires_grad_(False)\n        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0.0)\n        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)",
        "detail": "my_work.Project 1 Vision Transformer.models.segmentation",
        "documentation": {}
    },
    {
        "label": "MaskHeadSmallConv",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.segmentation",
        "description": "my_work.Project 1 Vision Transformer.models.segmentation",
        "peekOfCode": "class MaskHeadSmallConv(nn.Module):\n    \"\"\"\n    Simple convolutional head, using group norm.\n    Upsampling is done using a FPN approach\n    \"\"\"\n    def __init__(self, dim, fpn_dims, context_dim):\n        super().__init__()\n        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n        self.gn1 = torch.nn.GroupNorm(8, dim)",
        "detail": "my_work.Project 1 Vision Transformer.models.segmentation",
        "documentation": {}
    },
    {
        "label": "MHAttentionMap",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.segmentation",
        "description": "my_work.Project 1 Vision Transformer.models.segmentation",
        "peekOfCode": "class MHAttentionMap(nn.Module):\n    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True):\n        super().__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(dropout)\n        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n        nn.init.zeros_(self.k_linear.bias)",
        "detail": "my_work.Project 1 Vision Transformer.models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessSegm",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.segmentation",
        "description": "my_work.Project 1 Vision Transformer.models.segmentation",
        "peekOfCode": "class PostProcessSegm(nn.Module):\n    def __init__(self, threshold=0.5):\n        super().__init__()\n        self.threshold = threshold\n    @torch.no_grad()\n    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n        assert len(orig_target_sizes) == len(max_target_sizes)\n        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n        outputs_masks = F.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)",
        "detail": "my_work.Project 1 Vision Transformer.models.segmentation",
        "documentation": {}
    },
    {
        "label": "PostProcessPanoptic",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.segmentation",
        "description": "my_work.Project 1 Vision Transformer.models.segmentation",
        "peekOfCode": "class PostProcessPanoptic(nn.Module):\n    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n    coco panoptic API \"\"\"\n    def __init__(self, is_thing_map, threshold=0.85):\n        \"\"\"\n        Parameters:\n           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n                          the class is  a thing (True) or a stuff (False) class\n           threshold: confidence threshold: segments with confidence lower than this will be deleted\n        \"\"\"",
        "detail": "my_work.Project 1 Vision Transformer.models.segmentation",
        "documentation": {}
    },
    {
        "label": "dice_loss",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.models.segmentation",
        "description": "my_work.Project 1 Vision Transformer.models.segmentation",
        "peekOfCode": "def dice_loss(inputs, targets, num_boxes):\n    \"\"\"\n    Compute the DICE loss, similar to generalized IOU for masks\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n    \"\"\"",
        "detail": "my_work.Project 1 Vision Transformer.models.segmentation",
        "documentation": {}
    },
    {
        "label": "sigmoid_focal_loss",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.models.segmentation",
        "description": "my_work.Project 1 Vision Transformer.models.segmentation",
        "peekOfCode": "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n    \"\"\"\n    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n        alpha: (optional) Weighting factor in range (0,1) to balance",
        "detail": "my_work.Project 1 Vision Transformer.models.segmentation",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.transformer",
        "description": "my_work.Project 1 Vision Transformer.models.transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False,\n                 return_intermediate_dec=False):\n        super().__init__()\n        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n                                                dropout, activation, normalize_before)\n        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)",
        "detail": "my_work.Project 1 Vision Transformer.models.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerEncoder",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.transformer",
        "description": "my_work.Project 1 Vision Transformer.models.transformer",
        "peekOfCode": "class TransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers, norm=None):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n    def forward(self, src,\n                mask: Optional[Tensor] = None,\n                src_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None):",
        "detail": "my_work.Project 1 Vision Transformer.models.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerDecoder",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.transformer",
        "description": "my_work.Project 1 Vision Transformer.models.transformer",
        "peekOfCode": "class TransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n        # 是否返回中间层 默认True  因为DETR默认6个Decoder都会返回结果，一起加入损失计算的\n        # 每一层Decoder都是逐层解析，逐层加强的，所以前面层的解析效果对后面层的解析是有意义的，所以作者把前面5层的输出也加入损失计算\n        self.return_intermediate = return_intermediate\n    def forward(self, tgt, memory,",
        "detail": "my_work.Project 1 Vision Transformer.models.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerEncoderLayer",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.transformer",
        "description": "my_work.Project 1 Vision Transformer.models.transformer",
        "peekOfCode": "class TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)",
        "detail": "my_work.Project 1 Vision Transformer.models.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerDecoderLayer",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.models.transformer",
        "description": "my_work.Project 1 Vision Transformer.models.transformer",
        "peekOfCode": "class TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)",
        "detail": "my_work.Project 1 Vision Transformer.models.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.models.transformer",
        "description": "my_work.Project 1 Vision Transformer.models.transformer",
        "peekOfCode": "def build_transformer(args):\n    return Transformer(\n        d_model=args.hidden_dim,\n        dropout=args.dropout,\n        nhead=args.nheads,\n        dim_feedforward=args.dim_feedforward,\n        num_encoder_layers=args.enc_layers,\n        num_decoder_layers=args.dec_layers,\n        normalize_before=args.pre_norm,\n        return_intermediate_dec=True,",
        "detail": "my_work.Project 1 Vision Transformer.models.transformer",
        "documentation": {}
    },
    {
        "label": "combine_to_panoptic_single_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "peekOfCode": "def combine_to_panoptic_single_core(proc_id, img_ids, img_id2img, inst_by_image,\n                                    sem_by_image, segmentations_folder, overlap_thr,\n                                    stuff_area_limit, categories):\n    panoptic_json = []\n    id_generator = IdGenerator(categories)\n    for idx, img_id in enumerate(img_ids):\n        img = img_id2img[img_id]\n        if idx % 100 == 0:\n            print('Core: {}, {} from {} images processed.'.format(proc_id, idx,\n                                                                  len(img_ids)))",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "documentation": {}
    },
    {
        "label": "combine_to_panoptic_multi_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "peekOfCode": "def combine_to_panoptic_multi_core(img_id2img, inst_by_image,\n                                   sem_by_image, segmentations_folder, overlap_thr,\n                                   stuff_area_limit, categories):\n    cpu_num = multiprocessing.cpu_count()\n    img_ids_split = np.array_split(list(img_id2img), cpu_num)\n    print(\"Number of cores: {}, images per core: {}\".format(cpu_num, len(img_ids_split[0])))\n    workers = multiprocessing.Pool(processes=cpu_num)\n    processes = []\n    for proc_id, img_ids in enumerate(img_ids_split):\n        p = workers.apply_async(combine_to_panoptic_single_core,",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "documentation": {}
    },
    {
        "label": "combine_predictions",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "peekOfCode": "def combine_predictions(semseg_json_file, instseg_json_file, images_json_file,\n                        categories_json_file, segmentations_folder,\n                        panoptic_json_file, confidence_thr, overlap_thr,\n                        stuff_area_limit):\n    start_time = time.time()\n    with open(semseg_json_file, 'r') as f:\n        sem_results = json.load(f)\n    with open(instseg_json_file, 'r') as f:\n        inst_results = json.load(f)\n    with open(images_json_file, 'r') as f:",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.combine_semantic_and_instance_predictions",
        "documentation": {}
    },
    {
        "label": "PQStatCat",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "peekOfCode": "class PQStatCat():\n        def __init__(self):\n            self.iou = 0.0\n            self.tp = 0\n            self.fp = 0\n            self.fn = 0\n        def __iadd__(self, pq_stat_cat):\n            self.iou += pq_stat_cat.iou\n            self.tp += pq_stat_cat.tp\n            self.fp += pq_stat_cat.fp",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "PQStat",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "peekOfCode": "class PQStat():\n    def __init__(self):\n        self.pq_per_cat = defaultdict(PQStatCat)\n    def __getitem__(self, i):\n        return self.pq_per_cat[i]\n    def __iadd__(self, pq_stat):\n        for label, pq_stat_cat in pq_stat.pq_per_cat.items():\n            self.pq_per_cat[label] += pq_stat_cat\n        return self\n    def pq_average(self, categories, isthing):",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "pq_compute_single_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "peekOfCode": "def pq_compute_single_core(proc_id, annotation_set, gt_folder, pred_folder, categories):\n    pq_stat = PQStat()\n    idx = 0\n    for gt_ann, pred_ann in annotation_set:\n        if idx % 100 == 0:\n            print('Core: {}, {} from {} images processed'.format(proc_id, idx, len(annotation_set)))\n        idx += 1\n        pan_gt = np.array(Image.open(os.path.join(gt_folder, gt_ann['file_name'])), dtype=np.uint32)\n        pan_gt = rgb2id(pan_gt)\n        pan_pred = np.array(Image.open(os.path.join(pred_folder, pred_ann['file_name'])), dtype=np.uint32)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "pq_compute_multi_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "peekOfCode": "def pq_compute_multi_core(matched_annotations_list, gt_folder, pred_folder, categories):\n    cpu_num = multiprocessing.cpu_count()\n    annotations_split = np.array_split(matched_annotations_list, cpu_num)\n    print(\"Number of cores: {}, images per core: {}\".format(cpu_num, len(annotations_split[0])))\n    workers = multiprocessing.Pool(processes=cpu_num)\n    processes = []\n    for proc_id, annotation_set in enumerate(annotations_split):\n        p = workers.apply_async(pq_compute_single_core,\n                                (proc_id, annotation_set, gt_folder, pred_folder, categories))\n        processes.append(p)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "pq_compute",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "peekOfCode": "def pq_compute(gt_json_file, pred_json_file, gt_folder=None, pred_folder=None):\n    start_time = time.time()\n    with open(gt_json_file, 'r') as f:\n        gt_json = json.load(f)\n    with open(pred_json_file, 'r') as f:\n        pred_json = json.load(f)\n    if gt_folder is None:\n        gt_folder = gt_json_file.replace('.json', '')\n    if pred_folder is None:\n        pred_folder = pred_json_file.replace('.json', '')",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "OFFSET",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "peekOfCode": "OFFSET = 256 * 256 * 256\nVOID = 0\nclass PQStatCat():\n        def __init__(self):\n            self.iou = 0.0\n            self.tp = 0\n            self.fp = 0\n            self.fn = 0\n        def __iadd__(self, pq_stat_cat):\n            self.iou += pq_stat_cat.iou",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "VOID",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "peekOfCode": "VOID = 0\nclass PQStatCat():\n        def __init__(self):\n            self.iou = 0.0\n            self.tp = 0\n            self.fp = 0\n            self.fn = 0\n        def __iadd__(self, pq_stat_cat):\n            self.iou += pq_stat_cat.iou\n            self.tp += pq_stat_cat.tp",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "IdGenerator",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "peekOfCode": "class IdGenerator():\n    '''\n    The class is designed to generate unique IDs that have meaningful RGB encoding.\n    Given semantic category unique ID will be generated and its RGB encoding will\n    have color close to the predefined semantic category color.\n    The RGB encoding used is ID = R * 256 * G + 256 * 256 + B.\n    Class constructor takes dictionary {id: category_info}, where all semantic\n    class ids are presented and category_info record is a dict with fields\n    'isthing' and 'color'\n    '''",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "get_traceback",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "peekOfCode": "def get_traceback(f):\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except Exception as e:\n            print('Caught exception in worker thread:')\n            traceback.print_exc()\n            raise e\n    return wrapper",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "peekOfCode": "def rgb2id(color):\n    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n        if color.dtype == np.uint8:\n            color = color.astype(np.int32)\n        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\ndef id2rgb(id_map):\n    if isinstance(id_map, np.ndarray):\n        id_map_copy = id_map.copy()\n        rgb_shape = tuple(list(id_map.shape) + [3])",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "id2rgb",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "peekOfCode": "def id2rgb(id_map):\n    if isinstance(id_map, np.ndarray):\n        id_map_copy = id_map.copy()\n        rgb_shape = tuple(list(id_map.shape) + [3])\n        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n        for i in range(3):\n            rgb_map[..., i] = id_map_copy % 256\n            id_map_copy //= 256\n        return rgb_map\n    color = []",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "peekOfCode": "def save_json(d, file):\n    with open(file, 'w') as f:\n        json.dump(d, f)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.build.lib.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "panoptic_converter",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "peekOfCode": "def panoptic_converter(original_format_folder, out_folder, out_file):\n    if not os.path.isdir(out_folder):\n        print(\"Creating folder {} for panoptic segmentation PNGs\".format(out_folder))\n        os.mkdir(out_folder)\n    categories = []\n    for idx, el in enumerate(labels):\n        if el.ignoreInEval:\n            continue\n        categories.append({'id': el.id,\n                           'name': el.name,",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "documentation": {}
    },
    {
        "label": "original_format_folder",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "peekOfCode": "original_format_folder = './gtFine/val/'\n# folder to store panoptic PNGs\nout_folder = './cityscapes_data/cityscapes_panoptic_val/'\n# json with segmentations information\nout_file = './cityscapes_data/cityscapes_panoptic_val.json'\ndef panoptic_converter(original_format_folder, out_folder, out_file):\n    if not os.path.isdir(out_folder):\n        print(\"Creating folder {} for panoptic segmentation PNGs\".format(out_folder))\n        os.mkdir(out_folder)\n    categories = []",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "documentation": {}
    },
    {
        "label": "out_folder",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "peekOfCode": "out_folder = './cityscapes_data/cityscapes_panoptic_val/'\n# json with segmentations information\nout_file = './cityscapes_data/cityscapes_panoptic_val.json'\ndef panoptic_converter(original_format_folder, out_folder, out_file):\n    if not os.path.isdir(out_folder):\n        print(\"Creating folder {} for panoptic segmentation PNGs\".format(out_folder))\n        os.mkdir(out_folder)\n    categories = []\n    for idx, el in enumerate(labels):\n        if el.ignoreInEval:",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "documentation": {}
    },
    {
        "label": "out_file",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "peekOfCode": "out_file = './cityscapes_data/cityscapes_panoptic_val.json'\ndef panoptic_converter(original_format_folder, out_folder, out_file):\n    if not os.path.isdir(out_folder):\n        print(\"Creating folder {} for panoptic segmentation PNGs\".format(out_folder))\n        os.mkdir(out_folder)\n    categories = []\n    for idx, el in enumerate(labels):\n        if el.ignoreInEval:\n            continue\n        categories.append({'id': el.id,",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.cityscapes_gt_converter.cityscapes_panoptic_converter",
        "documentation": {}
    },
    {
        "label": "convert_single_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "peekOfCode": "def convert_single_core(proc_id, image_set, categories, source_folder, segmentations_folder, VOID=0):\n    annotations = []\n    for working_idx, image_info in enumerate(image_set):\n        if working_idx % 100 == 0:\n            print('Core: {}, {} from {} images converted'.format(proc_id, working_idx, len(image_set)))\n        file_name = '{}.png'.format(image_info['file_name'].rsplit('.')[0])\n        try:\n            original_format = np.array(Image.open(os.path.join(source_folder, file_name)), dtype=np.uint32)\n        except IOError:\n            raise KeyError('no prediction png file for id: {}'.format(image_info['id']))",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "documentation": {}
    },
    {
        "label": "converter",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "peekOfCode": "def converter(source_folder, images_json_file, categories_json_file,\n              segmentations_folder, predictions_json_file,\n              VOID=0):\n    start_time = time.time()\n    print(\"Reading image set information from {}\".format(images_json_file))\n    with open(images_json_file, 'r') as f:\n        d_coco = json.load(f)\n    images = d_coco['images']\n    with open(categories_json_file, 'r') as f:\n        categories_coco = json.load(f)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "documentation": {}
    },
    {
        "label": "OFFSET",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "peekOfCode": "OFFSET = 1000\n@get_traceback\ndef convert_single_core(proc_id, image_set, categories, source_folder, segmentations_folder, VOID=0):\n    annotations = []\n    for working_idx, image_info in enumerate(image_set):\n        if working_idx % 100 == 0:\n            print('Core: {}, {} from {} images converted'.format(proc_id, working_idx, len(image_set)))\n        file_name = '{}.png'.format(image_info['file_name'].rsplit('.')[0])\n        try:\n            original_format = np.array(Image.open(os.path.join(source_folder, file_name)), dtype=np.uint32)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.2channels2panoptic_coco_format",
        "documentation": {}
    },
    {
        "label": "convert_detection_to_panoptic_coco_format_single_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.detection2panoptic_coco_format",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.detection2panoptic_coco_format",
        "peekOfCode": "def convert_detection_to_panoptic_coco_format_single_core(\n    proc_id, coco_detection, img_ids, categories, segmentations_folder\n):\n    id_generator = IdGenerator(categories)\n    annotations_panoptic = []\n    for working_idx, img_id in enumerate(img_ids):\n        if working_idx % 100 == 0:\n            print('Core: {}, {} from {} images processed'.format(proc_id,\n                                                                 working_idx,\n                                                                 len(img_ids)))",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.detection2panoptic_coco_format",
        "documentation": {}
    },
    {
        "label": "convert_detection_to_panoptic_coco_format",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.detection2panoptic_coco_format",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.detection2panoptic_coco_format",
        "peekOfCode": "def convert_detection_to_panoptic_coco_format(input_json_file,\n                                              segmentations_folder,\n                                              output_json_file,\n                                              categories_json_file):\n    start_time = time.time()\n    if segmentations_folder is None:\n        segmentations_folder = output_json_file.rsplit('.', 1)[0]\n    if not os.path.isdir(segmentations_folder):\n        print(\"Creating folder {} for panoptic segmentation PNGs\".format(segmentations_folder))\n        os.mkdir(segmentations_folder)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.detection2panoptic_coco_format",
        "documentation": {}
    },
    {
        "label": "convert_panoptic_to_detection_coco_format_single_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2detection_coco_format",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2detection_coco_format",
        "peekOfCode": "def convert_panoptic_to_detection_coco_format_single_core(\n    proc_id, annotations_set, categories, segmentations_folder, things_only\n):\n    annotations_detection = []\n    for working_idx, annotation in enumerate(annotations_set):\n        if working_idx % 100 == 0:\n            print('Core: {}, {} from {} images processed'.format(proc_id,\n                                                                 working_idx,\n                                                                 len(annotations_set)))\n        file_name = '{}.png'.format(annotation['file_name'].rsplit('.')[0])",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2detection_coco_format",
        "documentation": {}
    },
    {
        "label": "convert_panoptic_to_detection_coco_format",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2detection_coco_format",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2detection_coco_format",
        "peekOfCode": "def convert_panoptic_to_detection_coco_format(input_json_file,\n                                              segmentations_folder,\n                                              output_json_file,\n                                              categories_json_file,\n                                              things_only):\n    start_time = time.time()\n    if segmentations_folder is None:\n        segmentations_folder = input_json_file.rsplit('.', 1)[0]\n    print(\"CONVERTING...\")\n    print(\"COCO panoptic format:\")",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2detection_coco_format",
        "documentation": {}
    },
    {
        "label": "extract_semantic_single_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "peekOfCode": "def extract_semantic_single_core(proc_id,\n                                 annotations_set,\n                                 segmentations_folder,\n                                 output_json_file,\n                                 semantic_seg_folder,\n                                 categories,\n                                 save_as_png,\n                                 things_other):\n    annotation_semantic_seg = []\n    for working_idx, annotation in enumerate(annotations_set):",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "documentation": {}
    },
    {
        "label": "extract_semantic",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "peekOfCode": "def extract_semantic(input_json_file,\n                     segmentations_folder,\n                     output_json_file,\n                     semantic_seg_folder,\n                     categories_json_file,\n                     things_other):\n    start_time = time.time()\n    with open(input_json_file, 'r') as f:\n        d_coco = json.load(f)\n    annotations = d_coco['annotations']",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "documentation": {}
    },
    {
        "label": "OTHER_CLASS_ID",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "peekOfCode": "OTHER_CLASS_ID = 183\n@get_traceback\ndef extract_semantic_single_core(proc_id,\n                                 annotations_set,\n                                 segmentations_folder,\n                                 output_json_file,\n                                 semantic_seg_folder,\n                                 categories,\n                                 save_as_png,\n                                 things_other):",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.converters.panoptic2semantic_segmentation",
        "documentation": {}
    },
    {
        "label": "combine_to_panoptic_single_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "peekOfCode": "def combine_to_panoptic_single_core(proc_id, img_ids, img_id2img, inst_by_image,\n                                    sem_by_image, segmentations_folder, overlap_thr,\n                                    stuff_area_limit, categories):\n    panoptic_json = []\n    id_generator = IdGenerator(categories)\n    for idx, img_id in enumerate(img_ids):\n        img = img_id2img[img_id]\n        if idx % 100 == 0:\n            print('Core: {}, {} from {} images processed.'.format(proc_id, idx,\n                                                                  len(img_ids)))",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "documentation": {}
    },
    {
        "label": "combine_to_panoptic_multi_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "peekOfCode": "def combine_to_panoptic_multi_core(img_id2img, inst_by_image,\n                                   sem_by_image, segmentations_folder, overlap_thr,\n                                   stuff_area_limit, categories):\n    cpu_num = multiprocessing.cpu_count()\n    img_ids_split = np.array_split(list(img_id2img), cpu_num)\n    print(\"Number of cores: {}, images per core: {}\".format(cpu_num, len(img_ids_split[0])))\n    workers = multiprocessing.Pool(processes=cpu_num)\n    processes = []\n    for proc_id, img_ids in enumerate(img_ids_split):\n        p = workers.apply_async(combine_to_panoptic_single_core,",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "documentation": {}
    },
    {
        "label": "combine_predictions",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "peekOfCode": "def combine_predictions(semseg_json_file, instseg_json_file, images_json_file,\n                        categories_json_file, segmentations_folder,\n                        panoptic_json_file, confidence_thr, overlap_thr,\n                        stuff_area_limit):\n    start_time = time.time()\n    with open(semseg_json_file, 'r') as f:\n        sem_results = json.load(f)\n    with open(instseg_json_file, 'r') as f:\n        inst_results = json.load(f)\n    with open(images_json_file, 'r') as f:",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.combine_semantic_and_instance_predictions",
        "documentation": {}
    },
    {
        "label": "PQStatCat",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "peekOfCode": "class PQStatCat():\n        def __init__(self):\n            self.iou = 0.0\n            self.tp = 0\n            self.fp = 0\n            self.fn = 0\n        def __iadd__(self, pq_stat_cat):\n            self.iou += pq_stat_cat.iou\n            self.tp += pq_stat_cat.tp\n            self.fp += pq_stat_cat.fp",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "PQStat",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "peekOfCode": "class PQStat():\n    def __init__(self):\n        self.pq_per_cat = defaultdict(PQStatCat)\n    def __getitem__(self, i):\n        return self.pq_per_cat[i]\n    def __iadd__(self, pq_stat):\n        for label, pq_stat_cat in pq_stat.pq_per_cat.items():\n            self.pq_per_cat[label] += pq_stat_cat\n        return self\n    def pq_average(self, categories, isthing):",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "pq_compute_single_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "peekOfCode": "def pq_compute_single_core(proc_id, annotation_set, gt_folder, pred_folder, categories):\n    pq_stat = PQStat()\n    idx = 0\n    for gt_ann, pred_ann in annotation_set:\n        if idx % 100 == 0:\n            print('Core: {}, {} from {} images processed'.format(proc_id, idx, len(annotation_set)))\n        idx += 1\n        pan_gt = np.array(Image.open(os.path.join(gt_folder, gt_ann['file_name'])), dtype=np.uint32)\n        pan_gt = rgb2id(pan_gt)\n        pan_pred = np.array(Image.open(os.path.join(pred_folder, pred_ann['file_name'])), dtype=np.uint32)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "pq_compute_multi_core",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "peekOfCode": "def pq_compute_multi_core(matched_annotations_list, gt_folder, pred_folder, categories):\n    cpu_num = multiprocessing.cpu_count()\n    annotations_split = np.array_split(matched_annotations_list, cpu_num)\n    print(\"Number of cores: {}, images per core: {}\".format(cpu_num, len(annotations_split[0])))\n    workers = multiprocessing.Pool(processes=cpu_num)\n    processes = []\n    for proc_id, annotation_set in enumerate(annotations_split):\n        p = workers.apply_async(pq_compute_single_core,\n                                (proc_id, annotation_set, gt_folder, pred_folder, categories))\n        processes.append(p)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "pq_compute",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "peekOfCode": "def pq_compute(gt_json_file, pred_json_file, gt_folder=None, pred_folder=None):\n    start_time = time.time()\n    with open(gt_json_file, 'r') as f:\n        gt_json = json.load(f)\n    with open(pred_json_file, 'r') as f:\n        pred_json = json.load(f)\n    if gt_folder is None:\n        gt_folder = gt_json_file.replace('.json', '')\n    if pred_folder is None:\n        pred_folder = pred_json_file.replace('.json', '')",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "OFFSET",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "peekOfCode": "OFFSET = 256 * 256 * 256\nVOID = 0\nclass PQStatCat():\n        def __init__(self):\n            self.iou = 0.0\n            self.tp = 0\n            self.fp = 0\n            self.fn = 0\n        def __iadd__(self, pq_stat_cat):\n            self.iou += pq_stat_cat.iou",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "VOID",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "peekOfCode": "VOID = 0\nclass PQStatCat():\n        def __init__(self):\n            self.iou = 0.0\n            self.tp = 0\n            self.fp = 0\n            self.fn = 0\n        def __iadd__(self, pq_stat_cat):\n            self.iou += pq_stat_cat.iou\n            self.tp += pq_stat_cat.tp",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.evaluation",
        "documentation": {}
    },
    {
        "label": "IdGenerator",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "peekOfCode": "class IdGenerator():\n    '''\n    The class is designed to generate unique IDs that have meaningful RGB encoding.\n    Given semantic category unique ID will be generated and its RGB encoding will\n    have color close to the predefined semantic category color.\n    The RGB encoding used is ID = R * 256 * G + 256 * 256 + B.\n    Class constructor takes dictionary {id: category_info}, where all semantic\n    class ids are presented and category_info record is a dict with fields\n    'isthing' and 'color'\n    '''",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "get_traceback",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "peekOfCode": "def get_traceback(f):\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except Exception as e:\n            print('Caught exception in worker thread:')\n            traceback.print_exc()\n            raise e\n    return wrapper",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "rgb2id",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "peekOfCode": "def rgb2id(color):\n    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n        if color.dtype == np.uint8:\n            color = color.astype(np.int32)\n        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\ndef id2rgb(id_map):\n    if isinstance(id_map, np.ndarray):\n        id_map_copy = id_map.copy()\n        rgb_shape = tuple(list(id_map.shape) + [3])",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "id2rgb",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "peekOfCode": "def id2rgb(id_map):\n    if isinstance(id_map, np.ndarray):\n        id_map_copy = id_map.copy()\n        rgb_shape = tuple(list(id_map.shape) + [3])\n        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n        for i in range(3):\n            rgb_map[..., i] = id_map_copy % 256\n            id_map_copy //= 256\n        return rgb_map\n    color = []",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "peekOfCode": "def save_json(d, file):\n    with open(file, 'w') as f:\n        json.dump(d, f)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.panopticapi.utils",
        "documentation": {}
    },
    {
        "label": "generate_new_colors",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "generate_new_colors = True\njson_file = './sample_data/panoptic_examples.json'\nsegmentations_folder = './sample_data/panoptic_examples/'\nimg_folder = './sample_data/input_images/'\npanoptic_coco_categories = './panoptic_coco_categories.json'\nwith open(json_file, 'r') as f:\n    coco_d = json.load(f)\nann = np.random.choice(coco_d['annotations'])\nwith open(panoptic_coco_categories, 'r') as f:\n    categories_list = json.load(f)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "json_file",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "json_file = './sample_data/panoptic_examples.json'\nsegmentations_folder = './sample_data/panoptic_examples/'\nimg_folder = './sample_data/input_images/'\npanoptic_coco_categories = './panoptic_coco_categories.json'\nwith open(json_file, 'r') as f:\n    coco_d = json.load(f)\nann = np.random.choice(coco_d['annotations'])\nwith open(panoptic_coco_categories, 'r') as f:\n    categories_list = json.load(f)\ncategegories = {category['id']: category for category in categories_list}",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "segmentations_folder",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "segmentations_folder = './sample_data/panoptic_examples/'\nimg_folder = './sample_data/input_images/'\npanoptic_coco_categories = './panoptic_coco_categories.json'\nwith open(json_file, 'r') as f:\n    coco_d = json.load(f)\nann = np.random.choice(coco_d['annotations'])\nwith open(panoptic_coco_categories, 'r') as f:\n    categories_list = json.load(f)\ncategegories = {category['id']: category for category in categories_list}\n# find input img that correspond to the annotation",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "img_folder",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "img_folder = './sample_data/input_images/'\npanoptic_coco_categories = './panoptic_coco_categories.json'\nwith open(json_file, 'r') as f:\n    coco_d = json.load(f)\nann = np.random.choice(coco_d['annotations'])\nwith open(panoptic_coco_categories, 'r') as f:\n    categories_list = json.load(f)\ncategegories = {category['id']: category for category in categories_list}\n# find input img that correspond to the annotation\nimg = None",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "panoptic_coco_categories",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "panoptic_coco_categories = './panoptic_coco_categories.json'\nwith open(json_file, 'r') as f:\n    coco_d = json.load(f)\nann = np.random.choice(coco_d['annotations'])\nwith open(panoptic_coco_categories, 'r') as f:\n    categories_list = json.load(f)\ncategegories = {category['id']: category for category in categories_list}\n# find input img that correspond to the annotation\nimg = None\nfor image_info in coco_d['images']:",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "ann",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "ann = np.random.choice(coco_d['annotations'])\nwith open(panoptic_coco_categories, 'r') as f:\n    categories_list = json.load(f)\ncategegories = {category['id']: category for category in categories_list}\n# find input img that correspond to the annotation\nimg = None\nfor image_info in coco_d['images']:\n    if image_info['id'] == ann['image_id']:\n        try:\n            img = np.array(",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "categegories",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "categegories = {category['id']: category for category in categories_list}\n# find input img that correspond to the annotation\nimg = None\nfor image_info in coco_d['images']:\n    if image_info['id'] == ann['image_id']:\n        try:\n            img = np.array(\n                Image.open(os.path.join(img_folder, image_info['file_name']))\n            )\n        except:",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "img = None\nfor image_info in coco_d['images']:\n    if image_info['id'] == ann['image_id']:\n        try:\n            img = np.array(\n                Image.open(os.path.join(img_folder, image_info['file_name']))\n            )\n        except:\n            print(\"Undable to find correspoding input image.\")\n        break",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "segmentation",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "segmentation = np.array(\n    Image.open(os.path.join(segmentations_folder, ann['file_name'])),\n    dtype=np.uint8\n)\nsegmentation_id = rgb2id(segmentation)\n# find segments boundaries\nboundaries = find_boundaries(segmentation_id, mode='thick')\nif generate_new_colors:\n    segmentation[:, :, :] = 0\n    color_generator = IdGenerator(categegories)",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "segmentation_id",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "segmentation_id = rgb2id(segmentation)\n# find segments boundaries\nboundaries = find_boundaries(segmentation_id, mode='thick')\nif generate_new_colors:\n    segmentation[:, :, :] = 0\n    color_generator = IdGenerator(categegories)\n    for segment_info in ann['segments_info']:\n        color = color_generator.get_color(segment_info['category_id'])\n        mask = segmentation_id == segment_info['id']\n        segmentation[mask] = color",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "boundaries",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "boundaries = find_boundaries(segmentation_id, mode='thick')\nif generate_new_colors:\n    segmentation[:, :, :] = 0\n    color_generator = IdGenerator(categegories)\n    for segment_info in ann['segments_info']:\n        color = color_generator.get_color(segment_info['category_id'])\n        mask = segmentation_id == segment_info['id']\n        segmentation[mask] = color\n# depict boundaries\nsegmentation[boundaries] = [0, 0, 0]",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "segmentation[boundaries]",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "description": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "peekOfCode": "segmentation[boundaries] = [0, 0, 0]\nif img is None:\n    plt.figure()\n    plt.imshow(segmentation)\n    plt.axis('off')\nelse:\n    plt.figure(figsize=(9, 5))\n    plt.subplot(121)\n    plt.imshow(img)\n    plt.axis('off')",
        "detail": "my_work.Project 1 Vision Transformer.panopticapi.visualization",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "mask = np.array([[0, 0, 1, 1, 1],[0, 0, 0, 1, 1]])\ny_embed = mask.cumsum(0)\nx_embed = mask.cumsum(1)\nprint(\"x_embed:\\n\", x_embed)\nprint(\"y_embed:\\n\", y_embed)\nnum_pos_feats = 6\ntemperature = 20\ndim_t = np.arange(num_pos_feats)\ndim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\nprint(\"dim_t:\\n\", dim_t)",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "y_embed",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "y_embed = mask.cumsum(0)\nx_embed = mask.cumsum(1)\nprint(\"x_embed:\\n\", x_embed)\nprint(\"y_embed:\\n\", y_embed)\nnum_pos_feats = 6\ntemperature = 20\ndim_t = np.arange(num_pos_feats)\ndim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\nprint(\"dim_t:\\n\", dim_t)\npos_x = x_embed[:, :, None] / dim_t",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "x_embed",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "x_embed = mask.cumsum(1)\nprint(\"x_embed:\\n\", x_embed)\nprint(\"y_embed:\\n\", y_embed)\nnum_pos_feats = 6\ntemperature = 20\ndim_t = np.arange(num_pos_feats)\ndim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\nprint(\"dim_t:\\n\", dim_t)\npos_x = x_embed[:, :, None] / dim_t\npos_y = y_embed[:, :, None] / dim_t",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "num_pos_feats",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "num_pos_feats = 6\ntemperature = 20\ndim_t = np.arange(num_pos_feats)\ndim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\nprint(\"dim_t:\\n\", dim_t)\npos_x = x_embed[:, :, None] / dim_t\npos_y = y_embed[:, :, None] / dim_t\nprint(\"x_embed shape: \", x_embed.shape)\nprint(\"pos shape: \", pos_x.shape)\nprint(\"pos_x:\\n\", pos_x)",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "temperature",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "temperature = 20\ndim_t = np.arange(num_pos_feats)\ndim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\nprint(\"dim_t:\\n\", dim_t)\npos_x = x_embed[:, :, None] / dim_t\npos_y = y_embed[:, :, None] / dim_t\nprint(\"x_embed shape: \", x_embed.shape)\nprint(\"pos shape: \", pos_x.shape)\nprint(\"pos_x:\\n\", pos_x)\n# print(\"pos_y:\\n\", pos_y)",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "dim_t",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "dim_t = np.arange(num_pos_feats)\ndim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\nprint(\"dim_t:\\n\", dim_t)\npos_x = x_embed[:, :, None] / dim_t\npos_y = y_embed[:, :, None] / dim_t\nprint(\"x_embed shape: \", x_embed.shape)\nprint(\"pos shape: \", pos_x.shape)\nprint(\"pos_x:\\n\", pos_x)\n# print(\"pos_y:\\n\", pos_y)",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "dim_t",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\nprint(\"dim_t:\\n\", dim_t)\npos_x = x_embed[:, :, None] / dim_t\npos_y = y_embed[:, :, None] / dim_t\nprint(\"x_embed shape: \", x_embed.shape)\nprint(\"pos shape: \", pos_x.shape)\nprint(\"pos_x:\\n\", pos_x)\n# print(\"pos_y:\\n\", pos_y)",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "pos_x",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "pos_x = x_embed[:, :, None] / dim_t\npos_y = y_embed[:, :, None] / dim_t\nprint(\"x_embed shape: \", x_embed.shape)\nprint(\"pos shape: \", pos_x.shape)\nprint(\"pos_x:\\n\", pos_x)\n# print(\"pos_y:\\n\", pos_y)",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "pos_y",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "description": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "peekOfCode": "pos_y = y_embed[:, :, None] / dim_t\nprint(\"x_embed shape: \", x_embed.shape)\nprint(\"pos shape: \", pos_x.shape)\nprint(\"pos_x:\\n\", pos_x)\n# print(\"pos_y:\\n\", pos_y)",
        "detail": "my_work.Project 1 Vision Transformer.test.test_posi_embed",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.box_ops",
        "description": "my_work.Project 1 Vision Transformer.util.box_ops",
        "peekOfCode": "def box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(-1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=-1)\ndef box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n         (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)",
        "detail": "my_work.Project 1 Vision Transformer.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_xyxy_to_cxcywh",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.box_ops",
        "description": "my_work.Project 1 Vision Transformer.util.box_ops",
        "peekOfCode": "def box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n         (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)\n# modified from torchvision to also return the union\ndef box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]",
        "detail": "my_work.Project 1 Vision Transformer.util.box_ops",
        "documentation": {}
    },
    {
        "label": "box_iou",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.box_ops",
        "description": "my_work.Project 1 Vision Transformer.util.box_ops",
        "peekOfCode": "def box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n    union = area1[:, None] + area2 - inter\n    iou = inter / union\n    return iou, union",
        "detail": "my_work.Project 1 Vision Transformer.util.box_ops",
        "documentation": {}
    },
    {
        "label": "generalized_box_iou",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.box_ops",
        "description": "my_work.Project 1 Vision Transformer.util.box_ops",
        "peekOfCode": "def generalized_box_iou(boxes1, boxes2):\n    \"\"\"\n    Generalized IoU from https://giou.stanford.edu/\n    The boxes should be in [x0, y0, x1, y1] format\n    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n    and M = len(boxes2)\n    \"\"\"\n    # degenerate boxes gives inf / nan results\n    # so do an early check\n    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()",
        "detail": "my_work.Project 1 Vision Transformer.util.box_ops",
        "documentation": {}
    },
    {
        "label": "masks_to_boxes",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.box_ops",
        "description": "my_work.Project 1 Vision Transformer.util.box_ops",
        "peekOfCode": "def masks_to_boxes(masks):\n    \"\"\"Compute the bounding boxes around the provided masks\n    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n    Returns a [N, 4] tensors, with the boxes in xyxy format\n    \"\"\"\n    if masks.numel() == 0:\n        return torch.zeros((0, 4), device=masks.device)\n    h, w = masks.shape[-2:]\n    y = torch.arange(0, h, dtype=torch.float)\n    x = torch.arange(0, w, dtype=torch.float)",
        "detail": "my_work.Project 1 Vision Transformer.util.box_ops",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "class SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "class NestedTensor(object):\n    def __init__(self, tensors, mask: Optional[Tensor]):\n        self.tensors = tensors\n        self.mask = mask\n    def to(self, device):\n        # type: (Device) -> NestedTensor # noqa\n        cast_tensor = self.tensors.to(device)\n        mask = self.mask\n        if mask is not None:\n            assert mask is not None",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def all_gather(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"\n    world_size = get_world_size()\n    if world_size == 1:",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "reduce_dict",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that all processes\n    have the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "get_sha",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    def _run(command):\n        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n    sha = 'N/A'\n    diff = \"clean\"\n    branch = 'N/A'\n    try:\n        sha = _run(['git', 'rev-parse', 'HEAD'])\n        subprocess.check_output(['git', 'diff'], cwd=cwd)",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "collate_fn",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def collate_fn(batch):\n    batch = list(zip(*batch))\n    batch[0] = nested_tensor_from_tensor_list(batch[0])\n    return tuple(batch)\ndef _max_by_axis(the_list):\n    # type: (List[List[int]]) -> List[int]\n    maxes = the_list[0]\n    for sublist in the_list[1:]:\n        for index, item in enumerate(sublist):\n            maxes[index] = max(maxes[index], item)",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "nested_tensor_from_tensor_list",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n    # TODO make this more general\n    if tensor_list[0].ndim == 3:\n        if torchvision._is_tracing():\n            # nested_tensor_from_tensor_list() does not export well to ONNX\n            # call _onnx_nested_tensor_from_tensor_list() instead\n            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n        # TODO make it support different-sized images\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "setup_for_distributed",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef is_main_process():\n    return get_rank() == 0",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "save_on_master",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    elif 'SLURM_PROCID' in os.environ:\n        args.rank = int(os.environ['SLURM_PROCID'])",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "init_distributed_mode",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    elif 'SLURM_PROCID' in os.environ:\n        args.rank = int(os.environ['SLURM_PROCID'])\n        args.gpu = args.rank % torch.cuda.device_count()\n    else:\n        print('Not using distributed mode')",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    if target.numel() == 0:\n        return [torch.zeros([], device=output.device)]\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.misc",
        "description": "my_work.Project 1 Vision Transformer.util.misc",
        "peekOfCode": "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n    \"\"\"\n    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n    This will eventually be supported natively by PyTorch, and this\n    class can go away.\n    \"\"\"\n    if version.parse(torchvision.__version__) < version.parse('0.7'):\n        if input.numel() > 0:\n            return torch.nn.functional.interpolate(",
        "detail": "my_work.Project 1 Vision Transformer.util.misc",
        "documentation": {}
    },
    {
        "label": "plot_logs",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.plot_utils",
        "description": "my_work.Project 1 Vision Transformer.util.plot_utils",
        "peekOfCode": "def plot_logs(logs, fields=('class_error', 'loss_bbox_unscaled', 'mAP'), ewm_col=0, log_name='log.txt'):\n    '''\n    Function to plot specific fields from training log(s). Plots both training and test results.\n    :: Inputs - logs = list containing Path objects, each pointing to individual dir with a log file\n              - fields = which results to plot from each log file - plots both training and test for each field.\n              - ewm_col = optional, which column to use as the exponential weighted smoothing of the plots\n              - log_name = optional, name of log file if different than default 'log.txt'.\n    :: Outputs - matplotlib plots of results in fields, color coded for each log file.\n               - solid lines are training results, dashed lines are test results.\n    '''",
        "detail": "my_work.Project 1 Vision Transformer.util.plot_utils",
        "documentation": {}
    },
    {
        "label": "plot_precision_recall",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.util.plot_utils",
        "description": "my_work.Project 1 Vision Transformer.util.plot_utils",
        "peekOfCode": "def plot_precision_recall(files, naming_scheme='iter'):\n    if naming_scheme == 'exp_id':\n        # name becomes exp_id\n        names = [f.parts[-3] for f in files]\n    elif naming_scheme == 'iter':\n        names = [f.stem for f in files]\n    else:\n        raise ValueError(f'not supported {naming_scheme}')\n    fig, axs = plt.subplots(ncols=2, figsize=(16, 5))\n    for f, color, name in zip(files, sns.color_palette(\"Blues\", n_colors=len(files)), names):",
        "detail": "my_work.Project 1 Vision Transformer.util.plot_utils",
        "documentation": {}
    },
    {
        "label": "train_one_epoch",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.engine",
        "description": "my_work.Project 1 Vision Transformer.engine",
        "peekOfCode": "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, max_norm: float = 0):\n    model.train()\n    criterion.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = 10",
        "detail": "my_work.Project 1 Vision Transformer.engine",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.engine",
        "description": "my_work.Project 1 Vision Transformer.engine",
        "peekOfCode": "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n    model.eval()\n    criterion.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n    header = 'Test:'\n    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n    panoptic_evaluator = None",
        "detail": "my_work.Project 1 Vision Transformer.engine",
        "documentation": {}
    },
    {
        "label": "detr_resnet50",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.hubconf",
        "description": "my_work.Project 1 Vision Transformer.hubconf",
        "peekOfCode": "def detr_resnet50(pretrained=False, num_classes=91, return_postprocessor=False):\n    \"\"\"\n    DETR R50 with 6 encoder and 6 decoder layers.\n    Achieves 42/62.4 AP/AP50 on COCO val5k.\n    \"\"\"\n    model = _make_detr(\"resnet50\", dilation=False, num_classes=num_classes)\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\", map_location=\"cpu\", check_hash=True\n        )",
        "detail": "my_work.Project 1 Vision Transformer.hubconf",
        "documentation": {}
    },
    {
        "label": "detr_resnet50_dc5",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.hubconf",
        "description": "my_work.Project 1 Vision Transformer.hubconf",
        "peekOfCode": "def detr_resnet50_dc5(pretrained=False, num_classes=91, return_postprocessor=False):\n    \"\"\"\n    DETR-DC5 R50 with 6 encoder and 6 decoder layers.\n    The last block of ResNet-50 has dilation to increase\n    output resolution.\n    Achieves 43.3/63.1 AP/AP50 on COCO val5k.\n    \"\"\"\n    model = _make_detr(\"resnet50\", dilation=True, num_classes=num_classes)\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(",
        "detail": "my_work.Project 1 Vision Transformer.hubconf",
        "documentation": {}
    },
    {
        "label": "detr_resnet101",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.hubconf",
        "description": "my_work.Project 1 Vision Transformer.hubconf",
        "peekOfCode": "def detr_resnet101(pretrained=False, num_classes=91, return_postprocessor=False):\n    \"\"\"\n    DETR-DC5 R101 with 6 encoder and 6 decoder layers.\n    Achieves 43.5/63.8 AP/AP50 on COCO val5k.\n    \"\"\"\n    model = _make_detr(\"resnet101\", dilation=False, num_classes=num_classes)\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth\", map_location=\"cpu\", check_hash=True\n        )",
        "detail": "my_work.Project 1 Vision Transformer.hubconf",
        "documentation": {}
    },
    {
        "label": "detr_resnet101_dc5",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.hubconf",
        "description": "my_work.Project 1 Vision Transformer.hubconf",
        "peekOfCode": "def detr_resnet101_dc5(pretrained=False, num_classes=91, return_postprocessor=False):\n    \"\"\"\n    DETR-DC5 R101 with 6 encoder and 6 decoder layers.\n    The last block of ResNet-101 has dilation to increase\n    output resolution.\n    Achieves 44.9/64.7 AP/AP50 on COCO val5k.\n    \"\"\"\n    model = _make_detr(\"resnet101\", dilation=True, num_classes=num_classes)\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(",
        "detail": "my_work.Project 1 Vision Transformer.hubconf",
        "documentation": {}
    },
    {
        "label": "detr_resnet50_panoptic",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.hubconf",
        "description": "my_work.Project 1 Vision Transformer.hubconf",
        "peekOfCode": "def detr_resnet50_panoptic(\n    pretrained=False, num_classes=250, threshold=0.85, return_postprocessor=False\n):\n    \"\"\"\n    DETR R50 with 6 encoder and 6 decoder layers.\n    Achieves 43.4 PQ on COCO val5k.\n   threshold is the minimum confidence required for keeping segments in the prediction\n    \"\"\"\n    model = _make_detr(\"resnet50\", dilation=False, num_classes=num_classes, mask=True)\n    is_thing_map = {i: i <= 90 for i in range(250)}",
        "detail": "my_work.Project 1 Vision Transformer.hubconf",
        "documentation": {}
    },
    {
        "label": "detr_resnet50_dc5_panoptic",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.hubconf",
        "description": "my_work.Project 1 Vision Transformer.hubconf",
        "peekOfCode": "def detr_resnet50_dc5_panoptic(\n    pretrained=False, num_classes=250, threshold=0.85, return_postprocessor=False\n):\n    \"\"\"\n    DETR-DC5 R50 with 6 encoder and 6 decoder layers.\n    The last block of ResNet-50 has dilation to increase\n    output resolution.\n    Achieves 44.6 on COCO val5k.\n   threshold is the minimum confidence required for keeping segments in the prediction\n    \"\"\"",
        "detail": "my_work.Project 1 Vision Transformer.hubconf",
        "documentation": {}
    },
    {
        "label": "detr_resnet101_panoptic",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.hubconf",
        "description": "my_work.Project 1 Vision Transformer.hubconf",
        "peekOfCode": "def detr_resnet101_panoptic(\n    pretrained=False, num_classes=250, threshold=0.85, return_postprocessor=False\n):\n    \"\"\"\n    DETR-DC5 R101 with 6 encoder and 6 decoder layers.\n    Achieves 45.1 PQ on COCO val5k.\n   threshold is the minimum confidence required for keeping segments in the prediction\n    \"\"\"\n    model = _make_detr(\"resnet101\", dilation=False, num_classes=num_classes, mask=True)\n    is_thing_map = {i: i <= 90 for i in range(250)}",
        "detail": "my_work.Project 1 Vision Transformer.hubconf",
        "documentation": {}
    },
    {
        "label": "dependencies",
        "kind": 5,
        "importPath": "my_work.Project 1 Vision Transformer.hubconf",
        "description": "my_work.Project 1 Vision Transformer.hubconf",
        "peekOfCode": "dependencies = [\"torch\", \"torchvision\"]\ndef _make_detr(backbone_name: str, dilation=False, num_classes=91, mask=False):\n    hidden_dim = 256\n    backbone = Backbone(backbone_name, train_backbone=True, return_interm_layers=mask, dilation=dilation)\n    pos_enc = PositionEmbeddingSine(hidden_dim // 2, normalize=True)\n    backbone_with_pos_enc = Joiner(backbone, pos_enc)\n    backbone_with_pos_enc.num_channels = backbone.num_channels\n    transformer = Transformer(d_model=hidden_dim, return_intermediate_dec=True)\n    detr = DETR(backbone_with_pos_enc, transformer, num_classes=num_classes, num_queries=100)\n    if mask:",
        "detail": "my_work.Project 1 Vision Transformer.hubconf",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.main",
        "description": "my_work.Project 1 Vision Transformer.main",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n    parser.add_argument('--lr', default=1e-4, type=float)\n    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n    parser.add_argument('--batch_size', default=2, type=int)\n    parser.add_argument('--weight_decay', default=1e-4, type=float)\n    parser.add_argument('--epochs', default=300, type=int)\n    parser.add_argument('--lr_drop', default=200, type=int)\n    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n                        help='gradient clipping max norm')",
        "detail": "my_work.Project 1 Vision Transformer.main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.main",
        "description": "my_work.Project 1 Vision Transformer.main",
        "peekOfCode": "def main(args):\n    utils.init_distributed_mode(args)\n    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n    if args.frozen_weights is not None:\n        assert args.masks, \"Frozen training is meant for segmentation only\"\n    print(args)\n    device = torch.device(args.device)\n    # fix the seed for reproducibility\n    seed = args.seed + utils.get_rank()\n    torch.manual_seed(seed)",
        "detail": "my_work.Project 1 Vision Transformer.main",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "description": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "peekOfCode": "class Trainer(object):\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        import main as detection\n        self._setup_gpu_args()\n        detection.main(self.args)\n    def checkpoint(self):\n        import os\n        import submitit",
        "detail": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "description": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "peekOfCode": "def parse_args():\n    detection_parser = detection.get_args_parser()\n    parser = argparse.ArgumentParser(\"Submitit for detection\", parents=[detection_parser])\n    parser.add_argument(\"--ngpus\", default=8, type=int, help=\"Number of gpus to request on each node\")\n    parser.add_argument(\"--nodes\", default=4, type=int, help=\"Number of nodes to request\")\n    parser.add_argument(\"--timeout\", default=60, type=int, help=\"Duration of the job\")\n    parser.add_argument(\"--job_dir\", default=\"\", type=str, help=\"Job dir. Leave empty for automatic.\")\n    return parser.parse_args()\ndef get_shared_folder() -> Path:\n    user = os.getenv(\"USER\")",
        "detail": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "documentation": {}
    },
    {
        "label": "get_shared_folder",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "description": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "peekOfCode": "def get_shared_folder() -> Path:\n    user = os.getenv(\"USER\")\n    if Path(\"/checkpoint/\").is_dir():\n        p = Path(f\"/checkpoint/{user}/experiments\")\n        p.mkdir(exist_ok=True)\n        return p\n    raise RuntimeError(\"No shared folder available\")\ndef get_init_file():\n    # Init file must not exist, but it's parent dir must exist.\n    os.makedirs(str(get_shared_folder()), exist_ok=True)",
        "detail": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "documentation": {}
    },
    {
        "label": "get_init_file",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "description": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "peekOfCode": "def get_init_file():\n    # Init file must not exist, but it's parent dir must exist.\n    os.makedirs(str(get_shared_folder()), exist_ok=True)\n    init_file = get_shared_folder() / f\"{uuid.uuid4().hex}_init\"\n    if init_file.exists():\n        os.remove(str(init_file))\n    return init_file\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args",
        "detail": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "description": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "peekOfCode": "def main():\n    args = parse_args()\n    if args.job_dir == \"\":\n        args.job_dir = get_shared_folder() / \"%j\"\n    # Note that the folder will depend on the job_id, to easily track experiments\n    executor = submitit.AutoExecutor(folder=args.job_dir, slurm_max_num_timeout=30)\n    # cluster setup is defined by environment variables\n    num_gpus_per_node = args.ngpus\n    nodes = args.nodes\n    timeout_min = args.timeout",
        "detail": "my_work.Project 1 Vision Transformer.run_with_submitit",
        "documentation": {}
    },
    {
        "label": "Tester",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.test_all",
        "description": "my_work.Project 1 Vision Transformer.test_all",
        "peekOfCode": "class Tester(unittest.TestCase):\n    def test_box_cxcywh_to_xyxy(self):\n        t = torch.rand(10, 4)\n        r = box_ops.box_xyxy_to_cxcywh(box_ops.box_cxcywh_to_xyxy(t))\n        self.assertLess((t - r).abs().max(), 1e-5)\n    @staticmethod\n    def indices_torch2python(indices):\n        return [(i.tolist(), j.tolist()) for i, j in indices]\n    def test_hungarian(self):\n        n_queries, n_targets, n_classes = 100, 15, 91",
        "detail": "my_work.Project 1 Vision Transformer.test_all",
        "documentation": {}
    },
    {
        "label": "ONNXExporterTester",
        "kind": 6,
        "importPath": "my_work.Project 1 Vision Transformer.test_all",
        "description": "my_work.Project 1 Vision Transformer.test_all",
        "peekOfCode": "class ONNXExporterTester(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        torch.manual_seed(123)\n    def run_model(self, model, inputs_list, tolerate_small_mismatch=False, do_constant_folding=True, dynamic_axes=None,\n                  output_names=None, input_names=None):\n        model.eval()\n        onnx_io = io.BytesIO()\n        # export to onnx with the first input\n        torch.onnx.export(model, inputs_list[0], onnx_io,",
        "detail": "my_work.Project 1 Vision Transformer.test_all",
        "documentation": {}
    },
    {
        "label": "CameraGeometry",
        "kind": 6,
        "importPath": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "description": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "peekOfCode": "class CameraGeometry(object):\n    def __init__(self, height=1.3, yaw_deg=0, pitch_deg=-5, roll_deg=0, image_width=1024, image_height=512, field_of_view_deg=45):\n        # scalar constants\n        self.height = height\n        self.pitch_deg = pitch_deg\n        self.roll_deg = roll_deg\n        self.yaw_deg = yaw_deg\n        self.image_width = image_width\n        self.image_height = image_height\n        self.field_of_view_deg = field_of_view_deg",
        "detail": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "documentation": {}
    },
    {
        "label": "get_intrinsic_matrix",
        "kind": 2,
        "importPath": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "description": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "peekOfCode": "def get_intrinsic_matrix(field_of_view_deg, image_width, image_height):\n    \"\"\"\n    Returns intrinsic matrix K.\n    \"\"\"\n    # For our Carla camera alpha_u = alpha_v = alpha\n    # alpha can be computed given the cameras field of view via\n    field_of_view_rad = field_of_view_deg * np.pi/180\n    alpha = (image_width / 2.0) / np.tan(field_of_view_rad / 2.)\n    Cu = image_width / 2.0\n    Cv = image_height / 2.0",
        "detail": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "documentation": {}
    },
    {
        "label": "project_polyline",
        "kind": 2,
        "importPath": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "description": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "peekOfCode": "def project_polyline(polyline_world, trafo_world_to_cam, K):\n    \"\"\"\n    Returns array uv which contains the pixel coordinates of the polyline.\n    Parameters\n    ----------\n    polyline_world : array_like, shape (M,3)\n        Each row of this array is a vertex (x,y,z) of the polyline.\n    trafo_world_to_cam : array_like, shape (4,4)\n        Transformation matrix, that maps vectors (x_world, y_world, z_world, 1) \n        to vectors (x_cam, y_cam, z_cam, 1).",
        "detail": "my_work.Project 2 IPM.code.exercises.camera_geometry",
        "documentation": {}
    }
]